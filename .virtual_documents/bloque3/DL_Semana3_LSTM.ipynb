import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers
import tensorflow.keras.backend as K

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import matplotlib.pyplot as plt

from pathlib import Path
import math





df_15min = pd.read_csv('../data/electricityloaddiagrams20112014/LD2011_2014.txt', sep=';', parse_dates=[0], index_col=0, decimal=',', dtype=np.float64)
df_15min.head()


df_15min.describe()


df_15min.shape


4*365*24*4 # years * days in year * hours of days * 15 minutes in one hour 


4*6





# Sort, ensure unique index
df_15min = df_15min.sort_index()
df_15min = df_15min[~df_15min.index.duplicated(keep="first")]


# Check frequency and repair to a full 15-min grid if needed
expected_freq = pd.infer_freq(df_15min.index[:8]) or "15T"
print("Inferred 15-min frequency (approx):", expected_freq)


full_idx = pd.date_range(df_15min.index.min(), df_15min.index.max(), freq="15T")
if len(full_idx) != len(df_15min.index):
    print("Reindexing to full 15-min grid and forward-filling missing values...")
    df_15min = df_15min.reindex(full_idx).ffill()


# Resample to hourly by summing the four 15-min slots (energy-like aggregation)
df_daily = df_15min.resample("1D").sum()

print("Hourly shape:", df_daily.shape)
df_daily.head(3)


df_daily


W = 15   # lookback window
H = 7    # forecast horizon

def make_windows(arr, window=W, horizon=H):
    X, y = [], []
    for i in range(len(arr) - window - horizon + 1):
        X.append(arr[i:i+window])
        y.append(arr[i+window:i+window+horizon].ravel())
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    return X, y

#X_train, y_train = make_windows(train_scaled, W, H)
#X_val, y_val     = make_windows(val_scaled, W, H)
#X_test, y_test   = make_windows(test_scaled, W, H)

#X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape





X = np.empty(shape=(0,W))
y = np.empty(shape=(0,H))

n_cols = len(df_daily.columns)

for i in range(n_cols):
    arr = df_daily.iloc[:,i].values
    X_, y_ = make_windows(arr, W, H)
    X = np.concatenate([X, X_], axis=0)
    y = np.concatenate([y, y_], axis=0)


X.shape


y.shape


series = X[400]
plt.plot(series)
plt.show()


permutation = np.random.permutation(len(X))

X = X[permutation]
y = y[permutation]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


scaler2 = StandardScaler()
y_train_scaled = scaler2.fit_transform(y_train)
y_val_scaled = scaler2.transform(y_val)
y_test_scaled = scaler2.transform(y_test)





def build_model(window=W, horizon=H):
    inputs = tf.keras.Input(shape=(window, 1))
    x = layers.LSTM(128, return_sequences=True)(inputs)
    x = layers.Dropout(0.2)(x)
    x = layers.LSTM(64)(x)
    x = layers.Dropout(0.2)(x)
    outputs = layers.Dense(horizon)(x)
    model = tf.keras.Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="mse",
        metrics=["mae"]
    )
    return model

model = build_model()
model.summary()





ckpt_dir = Path("checkpoints"); ckpt_dir.mkdir(exist_ok=True)
ckpt_path = str(ckpt_dir / "best.keras")

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor="val_loss"),
    tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_best_only=True, monitor="val_loss")
]


ckpt_path


history = model.fit(
    X_train_scaled, y_train_scaled,
    validation_data=(X_val_scaled, y_val_scaled),
    epochs=50,
    batch_size=256,
    callbacks=callbacks,
    verbose=1
)


pd.DataFrame(history.history).plot(title="Training History")
plt.show()





y_pred_scaled = model.predict(X_test_scaled, verbose=0)


y_test_scaled.shape


# Back to original units (kW)
y_test_orig = scaler2.inverse_transform(y_test_scaled)
y_pred_orig = scaler2.inverse_transform(y_pred_scaled)


def mape(y_true, y_pred, eps=1e-6):
    denom = np.clip(np.abs(y_true), eps, None)
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0

def smape(y_true, y_pred, eps=1e-6):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denom = np.clip(denom, eps, None)
    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0



yt = y_test_orig.reshape(-1)
yp = y_pred_orig.reshape(-1)


metrics = {}
metrics["MAE"] = float(mean_absolute_error(yt, yp))
metrics["RMSE"] = float(math.sqrt(mean_squared_error(yt, yp)))
metrics["MAPE_%"] = float(mape(yt, yp))
metrics["sMAPE_%"] = float(smape(yt, yp))
metrics["R2"] = float(r2_score(yt, yp))


results_df = pd.DataFrame(metrics, index=["LSTM"]).T
results_df


print(W, H)


len(X_train)


sample_idx = 1000 
past = X_test[sample_idx]
truth = y_test_orig[sample_idx]
preds = y_pred_orig[sample_idx]


plt.figure()
plt.plot(range(-len(past), 0), past, label="history")
plt.plot(range(0, len(truth)), truth, label="truth")
plt.plot(range(0, len(preds)), preds, label="forecast")
plt.title(f"7 days Forecast")
plt.xlabel("Hours from forecast start")
plt.ylabel("kW")
plt.legend()
plt.show()


sample_idx = 65000 
past = X_test[sample_idx]
truth = y_test_orig[sample_idx]
preds = y_pred_orig[sample_idx]


plt.figure()
plt.plot(range(-len(past), 0), past, label="history")
plt.plot(range(0, len(truth)), truth, label="truth")
plt.plot(range(0, len(preds)), preds, label="forecast")
plt.title(f"7 days Forecast")
plt.xlabel("Hours from forecast start")
plt.ylabel("kW")
plt.legend()
plt.show()



