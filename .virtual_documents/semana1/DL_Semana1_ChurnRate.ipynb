import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers
import tensorflow.keras.backend as K





DATA_PATH = "../data/WA_Fn-UseC_-Telco-Customer-Churn.csv"
df = pd.read_csv(DATA_PATH)





print(df.shape)
print(df.dtypes)
print(df.isna().sum())
print(df["Churn"].value_counts())





# (a) Borrardo de variables irrelevantes
df = df.drop(columns=["customerID"])


# (b) Convertir total charges a numeric
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")


# (c) Definir target and predictores
target_col = "Churn"
y = (df[target_col] == "Yes").astype(int).values  # binary 0/1

# Separar features
X = df.drop(columns=[target_col])

# Identificar variables numericas vs categoricas
num_features = ["tenure", "MonthlyCharges", "TotalCharges"]
cat_features = [c for c in X.columns if c not in num_features]


df





# Imputación de missing values
num_imputer = SimpleImputer(strategy="median")
# Para categorías, rellenamos los valores faltantes con un valor especial.
cat_imputer = SimpleImputer(strategy="constant", fill_value="missing")


# Encoders / scalers
num_scaler = StandardScaler()
# OneHotEncoder 
cat_encoder = OneHotEncoder(handle_unknown="ignore")



preprocessor = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imputer", num_imputer), ("scaler", num_scaler)]), num_features),
        ("cat", Pipeline([("imputer", cat_imputer), ("ohe", cat_encoder)]), cat_features),
    ],
    remainder="drop",  # drop any other columns
)





X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full
)


# Ajustar preprocessing en el conjunto training
preprocessor.fit(X_train)

# Transformamos
X_train_prep = preprocessor.transform(X_train)
X_val_prep = preprocessor.transform(X_val)
X_test_prep = preprocessor.transform(X_test)

input_dim = X_train_prep.shape[1]





def make_model(input_dim, dropout_rate=0.3, hidden_units=[64, 32]):
    inputs = layers.Input(shape=(input_dim,))
    x = inputs
    for units in hidden_units:
        x = layers.Dense(units, activation="relu")(x)
        x = layers.Dropout(dropout_rate)(x)
    outputs = layers.Dense(1, activation="sigmoid")(x)
    model = models.Model(inputs=inputs, outputs=outputs)
    return model

model = make_model(input_dim=input_dim, dropout_rate=0.3, hidden_units=[64, 64, 32])


def f1_metric(y_true, y_pred):
    """Compute F1 for binary classification as a custom metric."""
    y_pred_bin = K.cast(K.greater(y_pred, 0.5), "int32")
    y_true_int = K.cast(y_true, "int32")
    tp = K.sum(K.cast(y_true_int * y_pred_bin, "float32"))
    fp = K.sum(K.cast((1 - y_true_int) * y_pred_bin, "float32"))
    fn = K.sum(K.cast(y_true_int * (1 - y_pred_bin), "float32"))
    precision = tp / (tp + fp + K.epsilon())
    recall = tp / (tp + fn + K.epsilon())
    f1 = 2 * precision * recall / (precision + recall + K.epsilon())
    return f1

model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-3),
    loss="binary_crossentropy",
    metrics=[f1_metric]
)






X_train_prep


early_stop = callbacks.EarlyStopping(
    monitor="val_f1_metric", mode="max", patience=10, restore_best_weights=True
)
reduce_lr = callbacks.ReduceLROnPlateau(
    monitor="val_f1_metric", mode="max", factor=0.5, patience=3
)

history = model.fit(
    X_train_pep := X_train_prep,
    y_train,
    validation_data=(X_val_prep, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=2
)





y_test_pred_prob = model.predict(X_test_prep).ravel()
y_test_pred = (y_test_pred_prob >= 0.5).astype(int)

print("Test F1:", f1_score(y_test, y_test_pred))
print(classification_report(y_test, y_test_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_test_pred))





model.save("../models/churn_model.h5")


import joblib
joblib.dump(preprocessor, "preprocessor.joblib")









